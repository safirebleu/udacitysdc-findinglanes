{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Lesson 1: Finding Lanes in Video\n",
    "by Tim Chen\n",
    "\n",
    "This project uses the concepts learned in the first section to identify lanes on the road.  \n",
    "\n",
    "We first start with the input frame which is an RGB image of (540, 960, 3):\n",
    "![](lesson1/figure1.png)\n",
    "\n",
    "It is then 'flattened' into a monochrome grayscale in order to simplify processing:\n",
    "![](lesson1/figure2.png)\n",
    "\n",
    "To remove some noise from the input, we use a Gaussian function to smooth the image to find large changes in pixel intensity that would indicate an edge. The following example has been exaggerated in order to visualize the smoothing effect (kernel=31):\n",
    "![](lesson1/figure3.png) \n",
    "\n",
    "Using moderate threshold values (Low=1, High=100), the Canny function give a typical 'edge-detected' image: \n",
    "![](lesson1/figure4-1.png)\n",
    "\n",
    "Tuning the threshold parameters (Low=150, High=200) to maximize high-contrasting regions, we can filter out more of the image in order to find areas of largest transitions or edges:\n",
    "![](lesson1/figure4-2.png)\n",
    "\n",
    "Next, a mask is created to remove areas (not in white) that will not be considered in detecting the space of the lane... \n",
    "![](lesson1/figure5.png)\n",
    "\n",
    "...it is then applied to the Canny image in order to isolate the lane markings within the masked region:\n",
    "![](lesson1/figure6.png)\n",
    "\n",
    "By applying the Hough Transform, will allows us to determine which pixels that fall roughly on a line. The function will determine if enough pixels form a line based on the threshold parameters declared at the beginning of the function. Pixels that don't meet the criteria for a line are filtered out and the remaining pixels are replaced with a line. Re-drawing the image with end-points of the lines will give us a clean image we can overlay:\n",
    "![](lesson1/figure7.png)\n",
    "\n",
    "The final result of overlaying the Hough Transform image over the original input image will verify if we've correctly 'detected the lane':\n",
    "![](lesson1/figure8.png)\n",
    "\n",
    "Overall, this gives a basic algorithm for detecting lanes. Because of its static masks and input parameters, it might be a useful algorithm for only keeping vehicles evenly within the lane. Dynamic range lighting processing and variable masks with variable parameters will help make this algorithm more effective in finding lanes in more complex situations.\n",
    "\n",
    "Below is the code to process the \"solidWhiteRight.mp4\" and \"solidYellowLeft.mp4\" videos to apply the algorithm above to detect the lanes on a straight highway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video white.mp4\n",
      "[MoviePy] Writing video white.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 221/222 [00:02<00:00, 84.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: white.mp4 \n",
      "\n",
      "Wall time: 2.89 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"white.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def process_image(image):\n",
    "    \n",
    "    #GRAYSCALE & NOISE FILTERING\n",
    "        #Function begins with taking the input image and converting it to grayscale \n",
    "    kernel_size = 5 #Set kernel size for Gaussian smoothing; Must be an odd number (3, 5, 7...)\n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) #Conversion to grayscale\n",
    "    blur_gray = cv2.GaussianBlur(gray,(kernel_size, kernel_size),0) #Gaussian smoothing function \n",
    "    \n",
    "    #EDGE DETECTION\n",
    "        # After 'flattening' the RGB image into grayscale and reducing noise, use the Canny function to detect edges. \n",
    "    # Threshold parameters for Canny edge detection:\n",
    "    low_threshold = 150\n",
    "    high_threshold = 200\n",
    "    edges = cv2.Canny(blur_gray, low_threshold, high_threshold) # Returns an image of contours/edges\n",
    "\n",
    "    #MASKING\n",
    "        # We now need to create a mask to define an area of where the lane is in the image. \n",
    "        # Ideally, this would be a dynamic mask, but for the purposes of this lesson, we create a static mask. \n",
    "    mask = np.zeros_like(edges)   \n",
    "    ignore_mask_color = 255   \n",
    "    \n",
    "        # We use cv2.fillPoly() to remove the regions where the lane cannot be by defining a four-sided mask\n",
    "    imshape = image.shape\n",
    "    vertices = np.array([[(0,imshape[0]),(450, 320), (500, 320), (imshape[1],imshape[0])]], dtype=np.int32)\n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color) # Fills verticies in the empty mask with a polygon\n",
    "    masked_edges = cv2.bitwise_and(edges, mask) # Processes the Edges image and removes pixels outside the mask\n",
    "\n",
    "    #HOUGH TRANSFORM\n",
    "        # Performing the hough transform will allow us to find pixels that are roughly congregated along in lines  \n",
    "    \n",
    "    # Hough transform parameters\n",
    "    rho = 1 # Distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180 # Angular resolution in radians of the Hough grid\n",
    "    threshold = 15 # Minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 120 # minimum number of pixels making up a line\n",
    "    max_line_gap = 100    # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(image)*0 # creating a blank to draw lines on\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),\n",
    "                                min_line_length, max_line_gap)\n",
    "\n",
    "    # Iterate over the output \"lines\" and draw lines on a blank image\n",
    "    left_lane = []\n",
    "    right_lane = []\n",
    "\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            m_i = (y2-y1)/(x2-x1)\n",
    "            if ( -.2 <= m_i <= .2 ):\n",
    "                break\n",
    "            elif(m_i < -.2):\n",
    "                left_lane.append([x1, y1, x2, y2])\n",
    "            else:\n",
    "                right_lane.append([x1, y1, x2, y2])\n",
    "\n",
    "    if not left_lane:\n",
    "        return image\n",
    "    elif not right_lane:\n",
    "        return image\n",
    "    else:\n",
    "        left = np.mean(left_lane,axis=0).astype(int)\n",
    "        right = np.mean(right_lane,axis=0).astype(int)\n",
    "\n",
    "        m1 = (left[3]-left[1])/(left[2]-left[0])\n",
    "        b1 = left[1]-(m1*left[0])\n",
    "\n",
    "        m2 = (right[3]-right[1])/(right[2]-right[0])\n",
    "        b2 = right[1]-(m2*right[0])\n",
    "\n",
    "        y_max = image.shape[0]\n",
    "        xL1_max = ((y_max - b1) / m1).astype(int)\n",
    "        xL2_max = ((vertices[0,1,1]-b1)/m1).astype(int)\n",
    "        xR1_max = ((y_max - b2) / m2).astype(int)\n",
    "        xR2_max = ((vertices[0,2,1]-b2)/m2).astype(int)\n",
    "\n",
    "\n",
    "        cv2.line(line_image,(xL1_max,y_max),(xL2_max,vertices[0,1,1]),(0,255,0),10)\n",
    "        cv2.line(line_image,(xR2_max,vertices[0,2,1]),(xR1_max,y_max),(255,0,0),10)\n",
    "\n",
    "        # Create a \"color\" binary image to combine with line image\n",
    "        color_edges = np.dstack((edges, edges, edges)) \n",
    "\n",
    "        # Draw the lines on the edge image\n",
    "        result = cv2.addWeighted(image, 1, line_image, 1, 0) \n",
    "\n",
    "        return result\n",
    "\n",
    "        \n",
    "%matplotlib inline\n",
    "\n",
    "white_output = 'white.mp4'\n",
    "clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))\n",
    "\n",
    "#HIT SHIFT-ENTER TO RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video yellow.mp4\n",
      "[MoviePy] Writing video yellow.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▉| 681/682 [00:07<00:00, 82.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: yellow.mp4 \n",
      "\n",
      "Wall time: 8.27 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"yellow.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_output = 'yellow.mp4'\n",
    "clip1 = VideoFileClip(\"solidYellowLeft.mp4\")\n",
    "yellow_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))\n",
    "\n",
    "#HIT SHIFT-ENTER TO RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## This concludes the homework assignment for Lesson 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
